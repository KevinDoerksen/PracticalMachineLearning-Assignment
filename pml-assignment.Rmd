# Practical Machine Learning Assignment
## Exercise Technique Prediction Algorithm

<hr>

### Summary

Machine learning is a valuable tool for data analysis in the information age.  We apply some machine learning techniques in R to the [Weight Lifting Exercise Technique](http://groupware.les.inf.puc-rio.br/har#weight_lifting_exercises) data set [1].

The Weight Lifting Exercise Technique data set measured six participants performing the Unilateral Dumbbell Biceps Curl and classified their technique into one of five classes:

A. Exactly according to specification

B. Common mistake: throwing the elbows to the front

C. Common mistake: lifting the dumbbell only halfway

D. Common mistake: lowering the dumbbell only halfway

E. Common mistake: throwing the hips to the front 

Our aim is to use their data set to construct a machine learning algorithm that can accurately predict the technique, given a set of input data.  We use a random forrest algorithm, produced using cross-validation with n = 4 to produce a prediction algorithm with an estimated out-of-sample error of only **0.33%**.


<hr>

### Data Collection

```{r packages, results='hide'}
### Required packages
suppressMessages(require(randomForest))
suppressMessages(require(caret))
suppressMessages(require(grDevices))
```

We begin by downloading the raw data files and extracting the training and testing data sets.

```{r download_raw_data, cache = TRUE}
if(!file.exists("./pml-training.csv")){
    download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv",
                  "pml-training.csv", method="libcurl")
}
if(!file.exists("./pml-testing.csv")){
    download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv",
                  "pml-testing.csv", method="libcurl")
}

pml_train <- read.csv("pml-training.csv", na.strings = c("","NA"))
pml_test <- read.csv("pml-testing.csv", na.strings = c("","NA"))
```


<hr>

### Data Subsetting

We now explore the training data set.

```{r}
dim(pml_train)
```

The training set is quite large with 19622 observations of 160 variables.

A quick peruse through the data set shows that there are a *lot* of NA values.  As these values can be a challenge to deal with in machine learning algorithms, we first investigate if we need even consider them.  

```{r}
c(sum(complete.cases(pml_train)), sum(colSums(is.na(pml_train)) == 0))
```

We see that only 406 of the 19622 rows are free from NA values, whereas 60 of the 160 columns are free from NA values.  

1. It is infeasible to fill in NA values in a column that only has 406/19622 non-NA values, so we cannot hope to reasonably fill in the missing data.

2. Reducing the data set from 19622 observations to 406 observations is excessive.  Besides, more observations generally means a better algorithm.

3. Reducing the potential pool of predictor variables from 160 down to 60 is reasonable.

For these reasons, we restrict our analysis to the columns that are free from NA values.  We are left considering the following 60 columns:

```{r}
colnames(pml_train)[colSums(is.na(pml_train)) == 0]
```

The first seven columns are of no interest to us and including them in our model increases the concern of overfitting.  Dropping these columns leaves us with 53 columns: 52 of which are potential predictor variables, and the last column, *classe*, is our outcome variable.  We subset our train and test datasets to only include these columns.  We add the *classe* variable from the training set and the *problem_id* variable from the testing set separately because those columns are unique to the respective data set.

```{r subset_columns, cache = TRUE}
Relevant_columns <- colnames(pml_train)[colSums(is.na(pml_train)) == 0][8:59]
pml_train_short <- pml_train[, Relevant_columns]
pml_test_short <- pml_test[, Relevant_columns]

pml_train_short <- cbind(pml_train_short, classe = pml_train$classe)
pml_test_short <- cbind(pml_test_short, problem_id = pml_test$problem_id)
```

From this point on, we set aside the test set until we have our prediction model.

<hr>

We partition the training set into two subsets: 

* 80% will go into the set *pmlTrain* and will be used to fit the model,

* 20% will go into the set *pmlVerify* and will be held back as our means of testing our model and estimating the out-of-sample error of our model.

```{r data_partition, cache = TRUE}
set.seed(1234)
inTrain <- createDataPartition(y=pml_train_short$classe, p=0.80, list=FALSE)
pmlTrain <- pml_train_short[inTrain,]
pmlVerify <- pml_train_short[-inTrain,]
dim(pmlTrain)
```


<hr>

### Exploratory Analysis

We would like to see the level of correlation between the different predictor variables.

```{r}
corrs <- cor(pml_train_short[1:52])
cc <- colorRampPalette(c("blue", "white", "red"))
heatmap(corrs, col=cc(256))
```

The blue colors represent negative correlation and the red colors represent positive correlation.  Darker colors indicate stronger correlation.  Notice that much of the heatmap is light blue and red in color, with much of it being nearly white.  This indicates that most of the predictor variables are relatively uncorrelated.  For this reason, we do not feel the need to preprocess our data with principal component analysis.

In addition, we would like to perform some exploratory plots for a handful of the variables.  For brevity-sake in this report, we only show the first 5 predictor variables against the classe variable.  We found similar results when plotting other subsets of variables against the classe variable.

```{r featureplot, cache = TRUE}
featurePlot(x=pml_train_short[1:5], y=pml_train_short$classe, plot="pairs")
```

Notice that many of the plots have nonlinear interactions.  On the other hand, the plots tend to be clusters of closely-related points.  This leads us to believe that tree-based algorithms would be good candidates for a prediction model.  We will therefore try using a random forrest algorithm to build our model.


<hr>

### Model Selection


We use a random forrest algorithm to model the data.  As a compromise between finding an accurate model and having a reasonable computation time, we have overridden the caret default of performing bootstrapping with n=25, instead using a cross-validation model with n=4.  This model was compiled once, then saved to a file finalModel.RData.  All subsequent runs used the saved file.

```{r eval=FALSE, echo=FALSE}
# set.seed(10000) #For reproducibility
# modFit2 <- randomForest(classe ~ ., data = pml_train_short, ntree = 100)
# modFit2
```

```{r eval=FALSE, echo=FALSE}
# set.seed(10000) #for reproducibility
# modFit <- train(classe ~ ., data=pml_train_short, method="rf", 
#                 trControl = trainControl(method = "cv", number = 4))
# save(modFit, file="modFit.RData")

### Once the above code has been run once, it suffices to reload the saved model.
load("modFit.RData")
modFit
```

```{r}

### Code to produce (and save) the final prediction model.

# set.seed(10000) #for reproducibility
# finalModel <- train(classe ~ ., data=pmlTrain, method="rf", 
#                     trControl = trainControl(method = "cv", number = 4))
# save(finalModel, file="finalModel.RData")

### Once the above code has been run once, it suffices to reload the saved model.

load("finalModel.RData")
finalModel
```



<hr>

### Model Evaluation

We now test our model on the *pmlVerify* data set to see how good a fit we have and to estimate the out of sample error for out algorithm.

```{r}
pred <- predict(finalModel, pmlVerify)
table(pred, pmlVerify$classe)
```

We have a very good fit with only a handful of misclassifications.  The accuracy of our model on this set is 99.67%:

```{r}
predRight <- pred==pmlVerify$classe
accuracy <- sum(as.numeric(predRight))/length(predRight)
accuracy*100
```

This gives us an **out of sample error rate** of 0.33%:

```{r}
(1 - accuracy)*100
```

When this model is applied to the actual test set, we produce a 100% success rate on the 20 samples.



<hr>

### References

1. Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. **Qualitative Activity Recognition of Weight Lifting Exercises.** Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.
